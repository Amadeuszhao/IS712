{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b4c161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/amadeus/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "INFO:transformers.file_utils:PyTorch version 1.11.0+cu102 available.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AdamW,T5ForConditionalGeneration,T5Tokenizer,get_linear_schedule_with_warmup\n",
    "class T5FineTuner(pl.LightningModule):\n",
    "  def __init__(self, hparams):\n",
    "    super(T5FineTuner, self).__init__()\n",
    "    self.hparams = hparams\n",
    "    \n",
    "    self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
    "    self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
    "  \n",
    "  def is_logger(self):\n",
    "    return self.trainer.proc_rank <= 0\n",
    "  \n",
    "  def forward(\n",
    "      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
    "  ):\n",
    "    return self.model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        decoder_attention_mask=decoder_attention_mask,\n",
    "        lm_labels=lm_labels,\n",
    "    )\n",
    "\n",
    "  def _step(self, batch):\n",
    "    lm_labels = batch[\"target_ids\"]\n",
    "    lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    outputs = self(\n",
    "        input_ids=batch[\"source_ids\"],\n",
    "        attention_mask=batch[\"source_mask\"],\n",
    "        lm_labels=lm_labels,\n",
    "        decoder_attention_mask=batch['target_mask']\n",
    "    )\n",
    "\n",
    "    loss = outputs[0]\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "\n",
    "    tensorboard_logs = {\"train_loss\": loss}\n",
    "    return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "  \n",
    "  def training_epoch_end(self, outputs):\n",
    "    avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "    tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "    return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "    return {\"val_loss\": loss}\n",
    "  \n",
    "  def validation_epoch_end(self, outputs):\n",
    "    avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "    tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "    return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "    model = self.model\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": self.hparams.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "    self.opt = optimizer\n",
    "    return [optimizer]\n",
    "  \n",
    "  def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "    if self.trainer.use_tpu:\n",
    "      xm.optimizer_step(optimizer)\n",
    "    else:\n",
    "      optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    self.lr_scheduler.step()\n",
    "  \n",
    "  def get_tqdm_dict(self):\n",
    "    tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "    return tqdm_dict\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
    "    dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n",
    "    t_total = (\n",
    "        (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "        // self.hparams.gradient_accumulation_steps\n",
    "        * float(self.hparams.num_train_epochs)\n",
    "    )\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "    self.lr_scheduler = scheduler\n",
    "    return dataloader\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams)\n",
    "    return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbfbeef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./content/train.csv') # 1179\n",
    "test_df = pd.read_csv('/data/test_phase_2_update.csv') # 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07084c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "for index, row in train_df.iterrows():\n",
    "  # print (row)\n",
    "  claim, evidence, label = row['Claim'], row['Evidence'], row['Label']\n",
    "  train_examples.append([claim.strip(), evidence.strip(), label])\n",
    "\n",
    "random.shuffle(train_examples)\n",
    "valid_num = int(len(train_examples)*0.1)\n",
    "valid_examples = train_examples[:valid_num]\n",
    "train_examples = train_examples[valid_num:]\n",
    "\n",
    "test_examples = []\n",
    "for index, row in test_df.iterrows():\n",
    "  # print (row)\n",
    "  claim, evidence = row['Claim'], row['Evidence']\n",
    "  test_examples.append([claim.strip(), evidence.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8665541",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_int = {\"NonInfo\":0,\"Supports\":1, 'Refutes':2}\n",
    "labels = ['NonInfo',\"Supports\",\"Refutes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4766ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactDataset(Dataset):\n",
    "  def __init__(self, tokenizer,data,  max_len=512):\n",
    "    \n",
    "    self.data = data\n",
    "    self.max_len = max_len\n",
    "    self.tokenizer = tokenizer\n",
    "    self.inputs = []\n",
    "    self.targets = []\n",
    "\n",
    "    self._build()\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.inputs)\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "    target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "    src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "    target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "    return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "  \n",
    "  def _build(self):\n",
    "    for simple in self.data:\n",
    "      if len(simple) == 3:\n",
    "        input_, target = \"Claim:\"+ simple[0] + \"Evidence:\" + simple[1], labels[simple[2]]    \n",
    "      else:\n",
    "        input_, target = \"Claim:\"+ simple[0] + \"Evidence:\" + simple[1], labels[0]    \n",
    "\n",
    "      input_ = input_ + ' </s>'\n",
    "      target = target + \" </s>\"\n",
    "\n",
    "       # tokenize inputs\n",
    "      tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "          [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "      )\n",
    "       # tokenize targets\n",
    "      tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "          [target], max_length=2, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "      )\n",
    "\n",
    "      self.inputs.append(tokenized_inputs)\n",
    "      self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc9fad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1062\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "train_dataset =FactDataset(tokenizer,train_examples, 512)\n",
    "print(len(train_dataset))\n",
    "valid_dataset = FactDataset(tokenizer,valid_examples, 512)\n",
    "print(len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f68b0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at /home/amadeus/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "for label in labels:\n",
    "  print(len(tokenizer.encode(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c0587c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_dir': 'fact_data', 'output_dir': '/data/t5_fact', 'model_name_or_path': 't5-base', 'tokenizer_name_or_path': 't5-base', 'max_seq_length': 512, 'learning_rate': 0.0003, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'train_batch_size': 8, 'eval_batch_size': 8, 'num_train_epochs': 20, 'gradient_accumulation_steps': 16, 'n_gpu': 1, 'early_stop_callback': False, 'fp_16': False, 'opt_level': 'O1', 'max_grad_norm': 1.0, 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "args_dict = dict(\n",
    "    data_dir=\"\", # path for data files\n",
    "    output_dir=\"\", # path to save the checkpoints\n",
    "    model_name_or_path='t5-base',\n",
    "    tokenizer_name_or_path='t5-base',\n",
    "    max_seq_length=512,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    n_gpu=1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "    seed=42,\n",
    ")\n",
    "args_dict.update({'data_dir': 'fact_data', 'output_dir': '/data/t5_fact', 'num_train_epochs':20})\n",
    "args = argparse.Namespace(**args_dict)\n",
    "print(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de6bbd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json from cache at /home/amadeus/.cache/torch/transformers/40578967d1f029acb6162b36db9d8b4307063e885990ccd297c2c5be1cf1b3d7.2995d650f5eba18c8baa4146e210d32d56165e90d374281741fc78b872cd6c9b\n",
      "INFO:transformers.configuration_utils:Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "INFO:transformers.file_utils:https://cdn.huggingface.co/t5-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /home/amadeus/.cache/torch/transformers/tmp_wxnksgy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62174828eb4a45c79da4f05a84c76976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.file_utils:storing https://cdn.huggingface.co/t5-base-pytorch_model.bin in cache at /home/amadeus/.cache/torch/transformers/f6f2fde9fa7611f4eff74620de9cbe734e7a717b5b143bd283cae4c2d6022990.54f906ff53bd09195cfc183a29cadc81b7705f07fcdb796d24163cb632b6bdfa\n",
      "INFO:transformers.file_utils:creating metadata file for /home/amadeus/.cache/torch/transformers/f6f2fde9fa7611f4eff74620de9cbe734e7a717b5b143bd283cae4c2d6022990.54f906ff53bd09195cfc183a29cadc81b7705f07fcdb796d24163cb632b6bdfa\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/t5-base-pytorch_model.bin from cache at /home/amadeus/.cache/torch/transformers/f6f2fde9fa7611f4eff74620de9cbe734e7a717b5b143bd283cae4c2d6022990.54f906ff53bd09195cfc183a29cadc81b7705f07fcdb796d24163cb632b6bdfa\n",
      "INFO:transformers.modeling_utils:Weights of T5ForConditionalGeneration not initialized from pretrained model: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at /home/amadeus/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n"
     ]
    }
   ],
   "source": [
    "model = T5FineTuner(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e028087",
   "metadata": {},
   "source": [
    "state_dict in:https://drive.google.com/file/d/1Cu4M8ca22jt_7aLwUkow5rt-XRGEgbT9/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a92754f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amadeus/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:82: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/t5_fact/checkpointepoch=7_v0.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1046\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1044\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1045\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1046\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1016\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1015\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1016\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39m_UntypedStorage)\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;66;03m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:176\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 176\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:152\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 152\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    154\u001b[0m             storage_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda, \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:136\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    133\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    137\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    138\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    139\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    140\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    141\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "state = torch.load(\"/data/t5_fact/checkpointepoch=7_v0.ckpt\")\n",
    "model.load_state_dict(state['state_dict'])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d4680a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e78afdee4d465885cf3e7c3c91ac79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = FactDataset(tokenizer, test_examples, 512)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=4)\n",
    "model.model.eval()\n",
    "outputs = []\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "  outs = model.model.generate(input_ids=batch['source_ids'].cuda(), \n",
    "                              attention_mask=batch['source_mask'].cuda(), \n",
    "                              max_length=2)\n",
    "\n",
    "  dec = [tokenizer.decode(ids) for ids in outs]\n",
    "  # print(len(dec))\n",
    "  \n",
    "  # print(len(target))\n",
    "  outputs.extend(dec)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50a85378",
   "metadata": {},
   "outputs": [],
   "source": [
    "results= []\n",
    "for i in outputs:\n",
    "  if i == \"Re\":\n",
    "    results.append(2)\n",
    "  elif i == \"Non\":\n",
    "    results.append(0)\n",
    "  elif i == \"Support\":\n",
    "    results.append(1)\n",
    "  else:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f7337",
   "metadata": {},
   "source": [
    "This is the result of original task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f8b7f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0643b3",
   "metadata": {},
   "source": [
    "state_dict in https://drive.google.com/file/d/1Wtos9PjCyASryzokSezfmaSMTbexLwYF/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65a41c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(\"/data/t5_info/checkpointepoch=9.ckpt\")\n",
    "model.load_state_dict(state['state_dict'])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f90a4b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d520712b504c9fbd3b1572eb638958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = FactDataset(tokenizer, test_examples, 512)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=4)\n",
    "model.model.eval()\n",
    "outputs = []\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "  outs = model.model.generate(input_ids=batch['source_ids'].cuda(), \n",
    "                              attention_mask=batch['source_mask'].cuda(), \n",
    "                              max_length=2)\n",
    "\n",
    "  dec = [tokenizer.decode(ids) for ids in outs]\n",
    "  # print(len(dec))\n",
    "  \n",
    "  # print(len(target))\n",
    "  outputs.extend(dec)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bfac1e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Non', 'Non', 'Info', 'Info', 'Info', 'Non', 'Non', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Non', 'Info', 'Non', 'Non', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Non', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Non', 'Non', 'Info', 'Non', 'Non', 'Info', 'Info', 'Info', 'Info', 'Non', 'Info', 'Info', 'Info', 'Non', 'Info', 'Info', 'Non', 'Info', 'Info', 'Info', 'Info', 'Non', 'Info', 'Non', 'Info', 'Info', 'Info', 'Info', 'Non', 'Non', 'Info', 'Info', 'Non', 'Non', 'Info', 'Info', 'Info', 'Info', 'Info', 'Non', 'Non', 'Info', 'Non', 'Info', 'Non', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Non', 'Non', 'Info', 'Non', 'Non', 'Info', 'Info', 'Info', 'Info', 'Info', 'Non', 'Info', 'Info', 'Non', 'Non', 'Info', 'Info', 'Info', 'Info', 'Non', 'Info', 'Info', 'Non', 'Info', 'Info', 'Info', 'Info', 'Non', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Info', 'Non', 'Info', 'Info', 'Info', 'Info', 'Non', 'Info', 'Info', 'Info', 'Non', 'Info', 'Info', 'Non', 'Info', 'Non', 'Info', 'Non', 'Info', 'Non', 'Non', 'Info']\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53ce81e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de05ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results= []\n",
    "for i in outputs:\n",
    "  if i == \"Non\":\n",
    "    results.append(0)\n",
    "  elif i == \"Info\":\n",
    "    results.append(1)\n",
    "  else:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89ab1fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f799d9b",
   "metadata": {},
   "source": [
    "state_dict in https://drive.google.com/file/d/1thnZqHIslJNid3ZJ-v4yfgfLSDGTm2Xl/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c775ade3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edecd48cde564732b330674ba6c323ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = torch.load(\"/data/t5_fact_check/checkpointcheckpoint_ckpt_epoch_10.ckpt\")\n",
    "model.load_state_dict(state['state_dict'])\n",
    "model = model.cuda()\n",
    "dataset = FactDataset(tokenizer, test_examples, 512)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=4)\n",
    "model.model.eval()\n",
    "semantic_outputs = []\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "  outs = model.model.generate(input_ids=batch['source_ids'].cuda(), \n",
    "                              attention_mask=batch['source_mask'].cuda(), \n",
    "                              max_length=2)\n",
    "\n",
    "  dec = [tokenizer.decode(ids) for ids in outs]\n",
    "  # print(len(dec))\n",
    "  \n",
    "  # print(len(target))\n",
    "  semantic_outputs.extend(dec)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98b90781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Re',\n",
       " 'Support',\n",
       " 'Support',\n",
       " 'Support']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1aec492",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_results= []\n",
    "for i in semantic_outputs:\n",
    "  if i == \"Re\":\n",
    "    semantic_results.append(2)\n",
    "  elif i == \"Support\":\n",
    "    semantic_results.append(1)\n",
    "  else:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa6660c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28696672",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results)):\n",
    "    if results[i] == 0:\n",
    "        continue\n",
    "    else:\n",
    "        results[i] = semantic_results[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18858956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19d7fab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2069b84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88f5af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results =[0,\n",
    " 0,\n",
    " 1,\n",
    " 1,\n",
    " 1,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 2,\n",
    " 2,\n",
    " 1,\n",
    " 2,\n",
    " 1,\n",
    " 2,\n",
    " 1,\n",
    " 1,\n",
    " 0,\n",
    " 2,\n",
    " 0,\n",
    " 0,\n",
    " 2,\n",
    " 1,\n",
    " 2,\n",
    " 1,\n",
    " 1,\n",
    " 2,\n",
    " 1,\n",
    " 1,\n",
    " 2,\n",
    " 0,\n",
    " 1,\n",
    " 2,\n",
    " 2,\n",
    " 1,\n",
    " 1,\n",
    " 1,\n",
    " 1,\n",
    " 2,\n",
    " 2,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 0,\n",
    " 2,\n",
    " 1,\n",
    " 1,\n",
    " 1,\n",
    " 2,\n",
    " 0,\n",
    " 1,\n",
    " 1,\n",
    " 1,\n",
    " 0,\n",
    " 1,\n",
    " 1,\n",
    " 0,\n",
    " 2,\n",
    " 2,\n",
    " 1,\n",
    " 1,\n",
    " 0,\n",
    " 1,\n",
    " 0,\n",
    " 2,\n",
    " 2,\n",
    " 1,\n",
    " 2,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 1,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 1,\n",
    " 1,\n",
    " 1,\n",
    " 2,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 0,\n",
    " 2,\n",
    " 0,\n",
    " 1,\n",
    " 2,\n",
    " 1,\n",
    " 1,\n",
    " 2,\n",
    " 1,\n",
    " 2,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 1,\n",
    " 0,\n",
    " 2,\n",
    " 2,\n",
    " 2,\n",
    " 2,\n",
    " 2,\n",
    " 0,\n",
    " 1,\n",
    " 1,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 1,\n",
    " 1,\n",
    " 1,\n",
    " 0,\n",
    " 1,\n",
    " 1,\n",
    " 0,\n",
    " 2,\n",
    " 2,\n",
    " 2,\n",
    " 2,\n",
    " 0,\n",
    " 1,\n",
    " 2,\n",
    " 1,\n",
    " 1,\n",
    " 1,\n",
    " 1,\n",
    " 1,\n",
    " 2,\n",
    " 2,\n",
    " 1,\n",
    " 0,\n",
    " 1,\n",
    " 2,\n",
    " 2,\n",
    " 1,\n",
    " 0,\n",
    " 2,\n",
    " 1,\n",
    " 1,\n",
    " 0,\n",
    " 1,\n",
    " 1,\n",
    " 0,\n",
    " 1,\n",
    " 0,\n",
    " 1,\n",
    " 0,\n",
    " 2,\n",
    " 0,\n",
    " 0,\n",
    " 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9db12b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results)):\n",
    "    if results[i] == 0:\n",
    "        continue\n",
    "    else:\n",
    "        results[i] = new_results[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae846cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6089af3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: nvidia-smi: not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8a59f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
